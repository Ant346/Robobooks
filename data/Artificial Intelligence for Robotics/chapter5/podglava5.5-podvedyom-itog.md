# 5.5 Подведём итог!

Нашей задачей в этой главе было использовать машинное обучение для обучения робота работе с его манипулятором/клешнёй. Мы использовали два метода с некоторыми вариациями. Мы использовали различные методы обучения, называемые Q-learning, чтобы разработать траекторию движения, выбирая отдельные действия в зависимости от состояния манипулятора/клешни робота. Каждое предложение оценивалось индивидуально как награда и как часть общего пути как ценность. В процессе обучения результаты обучения сохраняются в матрице Qmatrix, которая может быть использована для создания пути. Мы улучшили наш первый срез в программе RL путем индексирования или кодирования движений из 27-элементного массива возможных комбинаций двигателей в число от 0 до 26, а также индексирования состояния робота в таблицу поиска состояния. Это привело к 40-кратному ускорению процесса обучения. Наш подход к Q-learning был сопряжен с большим количеством состояний, в которых может находиться манипулятор/клешня робота.

Нашей второй методикой был генетический алгоритм. Мы создали индивидуальные случайные пути для создания популяции. Мы создали фитнес-функцию, чтобы забить каждый путь против нашей цели, и сохранили лучших исполнителей из каждого поколения. Затем мы скрестили генетический материал двух случайно выбранных индивидуумов, чтобы создать новый дочерний путь. Генетический алгоритм также моделирует мутацию, имея небольшой шанс случайного изменения шагов на пути. Результаты генетического алгоритма не имели проблем с пространством состояний манипулятора/клешни и выбирали путь всего через несколько поколений.

